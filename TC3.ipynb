{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nunesjoab/tech_challenge_4/blob/main/TC3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7RBSgMYcF1m",
        "outputId": "03214693-2e11-4ccf-8e53-cb1c0508b514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon May 26 10:19:30 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   29C    P0             44W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tlu5XhBEb9_Y",
        "outputId": "705d2ed1-feb8-4af5-d848-792d7fd1c388"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tTIgblXXQ4UA",
        "outputId": "0c170d00-de47-4a19-f77b-1a538cb1a883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "\n",
            "Menu:\n",
            "1. Carregar dados\n",
            "2. Filtrar e tratar dados\n",
            "3. Inicializar modelo BERT\n",
            "4. Executar previsÃµes iniciais\n",
            "5. Fine-tuning do modelo\n",
            "6. Executar previsÃµes refinadas\n",
            "0. Sair\n",
            "\n",
            "Exemplos dos dados carregados:\n",
            "\n",
            "Primeiras 5 linhas do conjunto de treinamento (raw):\n",
            "{'uid': '0000031909', 'title': 'Girls Ballet Tutu Neon Pink', 'content': 'High quality 3 layer ballet tutu. 12 inches in length', 'target_ind': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 111], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '0000032034', 'title': 'Adult Ballet Tutu Yellow', 'content': '', 'target_ind': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 16, 33, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '0000913154', 'title': 'The Way Things Work: An Illustrated Encyclopedia of Technology', 'content': '', 'target_ind': [116, 117, 118, 119, 120, 121, 122], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '0001360000', 'title': \"Mog's Kittens\", 'content': 'Judith Kerr&#8217;s best&#8211;selling adventures of that endearing (and exasperating) cat Mog have entertained children for more than 30 years. Now, even infants and toddlers can enjoy meeting this loveable feline. These sturdy little board books&#8212;with their bright, simple pictures, easy text, and hand&#8211;friendly formats&#8212;are just the thing to delight the very young. Ages 6 months&#8211;2 years.', 'target_ind': [146, 147, 148, 149, 495], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '0001381245', 'title': 'Misty of Chincoteague', 'content': '', 'target_ind': [151], 'target_rel': [1.0]}\n",
            "\n",
            "Primeiras 5 linhas do conjunto de teste:\n",
            "{'uid': '0000032069', 'title': 'Adult Ballet Tutu Cheetah Pink', 'content': '', 'target_ind': [0, 1, 2, 4, 7, 8], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '0000589012', 'title': \"Why Don't They Just Quit? DVD Roundtable Discussion: What Families and Friends need to Know About Addiction and Recovery\", 'content': '', 'target_ind': [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '0000031852', 'title': 'Girls Ballet Tutu Zebra Hot Pink', 'content': 'TUtu', 'target_ind': [13, 16, 18, 20, 23, 32, 33, 113, 115], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '0000032050', 'title': 'Adult Ballet Tutu Purple', 'content': '', 'target_ind': [1, 2, 4, 7, 8, 31, 32, 35, 41, 46, 53, 60, 61, 123, 124, 131, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '0001203088', 'title': \"Hilda Boswell's Omnibus - A Treasury of Favorites\", 'content': '', 'target_ind': [150], 'target_rel': [1.0]}\n",
            "\n",
            "Menu:\n",
            "1. Carregar dados\n",
            "2. Filtrar e tratar dados\n",
            "3. Inicializar modelo BERT\n",
            "4. Executar previsÃµes iniciais\n",
            "5. Fine-tuning do modelo\n",
            "6. Executar previsÃµes refinadas\n",
            "0. Sair\n",
            "\n",
            "Exemplos dos dados tratados:\n",
            "{'uid': '0000031909', 'title': 'Girls Ballet Tutu Neon Pink', 'content': 'High quality 3 layer ballet tutu. 12 inches in length', 'target_ind': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 111], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '0001360000', 'title': \"Mog's Kittens\", 'content': 'Judith Kerrâ€™s bestâ€“selling adventures of that endearing (and exasperating) cat Mog have entertained children for more than 30 years. Now, even infants and toddlers can enjoy meeting this loveable feline. These sturdy little board booksâ€”with their bright, simple pictures, easy text, and handâ€“friendly formatsâ€”are just the thing to delight the very young. Ages 6 monthsâ€“2 years.', 'target_ind': [146, 147, 148, 149, 495], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '0000031895', 'title': 'Girls Ballet Tutu Neon Blue', 'content': 'Dance tutu for girls ages 2-8 years. Perfect for dance practice, recitals and performances, costumes or just for fun!', 'target_ind': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 27, 31, 33, 42, 46, 54, 58, 111, 113, 125, 126, 159, 163, 202, 203, 204, 205, 206, 207], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '000100039X', 'title': 'The Prophet', 'content': 'In a distant, timeless place, a mysterious prophet walks the sands. At the moment of his departure, he wishes to offer the people gifts but possesses nothing. The people gather round, each asks a question of the heart, and the man\\'s wisdom is his gift. It is Gibran\\'s gift to us, as well, for Gibran\\'s prophet is rivaled in his wisdom only by the founders of the world\\'s great religions. On the most basic topics--marriage, children, friendship, work, pleasure--his words have a power and lucidity that in another era would surely have provoked the description \"divinely inspired.\" Free of dogma, free of power structures and metaphysics, consider these poetic, moving aphorisms a 20th-century supplement to all sacred traditions--as millions of other readers already have.--Brian Bruya--This text refers to theHardcoveredition.', 'target_ind': [329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '0001473905', 'title': 'Rightly Dividing the Word', 'content': '--This text refers to thePaperbackedition.', 'target_ind': [181, 182, 307, 380, 381], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "\n",
            "Primeiras 5 linhas do conjunto de teste:\n",
            "{'uid': '0000032069', 'title': 'Adult Ballet Tutu Cheetah Pink', 'content': '', 'target_ind': [0, 1, 2, 4, 7, 8], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '0000589012', 'title': \"Why Don't They Just Quit? DVD Roundtable Discussion: What Families and Friends need to Know About Addiction and Recovery\", 'content': '', 'target_ind': [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '0000031852', 'title': 'Girls Ballet Tutu Zebra Hot Pink', 'content': 'TUtu', 'target_ind': [13, 16, 18, 20, 23, 32, 33, 113, 115], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '0000032050', 'title': 'Adult Ballet Tutu Purple', 'content': '', 'target_ind': [1, 2, 4, 7, 8, 31, 32, 35, 41, 46, 53, 60, 61, 123, 124, 131, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "{'uid': '0001203088', 'title': \"Hilda Boswell's Omnibus - A Treasury of Favorites\", 'content': '', 'target_ind': [150], 'target_rel': [1.0]}\n",
            "\n",
            "Menu:\n",
            "1. Carregar dados\n",
            "2. Filtrar e tratar dados\n",
            "3. Inicializar modelo BERT\n",
            "4. Executar previsÃµes iniciais\n",
            "5. Fine-tuning do modelo\n",
            "6. Executar previsÃµes refinadas\n",
            "0. Sair\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo BERT inicializado com sucesso.\n",
            "\n",
            "Menu:\n",
            "1. Carregar dados\n",
            "2. Filtrar e tratar dados\n",
            "3. Inicializar modelo BERT\n",
            "4. Executar previsÃµes iniciais\n",
            "5. Fine-tuning do modelo\n",
            "6. Executar previsÃµes refinadas\n",
            "0. Sair\n",
            "\n",
            "Resultados do prompt:\n",
            "Pergunta: What is Girls Ballet Tutu Neon Pink?\n",
            "Resposta prevista: âŒ Incorreta\n",
            "\n",
            "Pergunta: What is Adult Ballet Tutu Yellow?\n",
            "Resposta prevista: âŒ Incorreta\n",
            "\n",
            "Pergunta: What is The Way Things Work: An Illustrated Encyclopedia of Technology?\n",
            "Resposta prevista: âŒ Incorreta\n",
            "\n",
            "Pergunta: What is Mog's Kittens?\n",
            "Resposta prevista: âŒ Incorreta\n",
            "\n",
            "Pergunta: What is Misty of Chincoteague?\n",
            "Resposta prevista: âŒ Incorreta\n",
            "\n",
            "\n",
            "Menu:\n",
            "1. Carregar dados\n",
            "2. Filtrar e tratar dados\n",
            "3. Inicializar modelo BERT\n",
            "4. Executar previsÃµes iniciais\n",
            "5. Fine-tuning do modelo\n",
            "6. Executar previsÃµes refinadas\n",
            "0. Sair\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='24992' max='24992' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [24992/24992 1:09:33, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.707100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.704700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.704000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.703600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.700300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.699700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.697600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.695900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.695700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.695500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.694800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.694700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.694300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.694500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.693900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.693300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Fine-tuning concluÃ­do em 77.61 minutos.\n",
            "Fine-tuning concluÃ­do.\n",
            "\n",
            "Resultados do prompt:\n",
            "Pergunta: What is Girls Ballet Tutu Neon Pink?\n",
            "Resposta prevista: âœ… Correta\n",
            "\n",
            "Pergunta: What is Adult Ballet Tutu Yellow?\n",
            "Resposta prevista: âœ… Correta\n",
            "\n",
            "Pergunta: What is The Way Things Work: An Illustrated Encyclopedia of Technology?\n",
            "Resposta prevista: âœ… Correta\n",
            "\n",
            "Pergunta: What is Mog's Kittens?\n",
            "Resposta prevista: âœ… Correta\n",
            "\n",
            "Pergunta: What is Misty of Chincoteague?\n",
            "Resposta prevista: âœ… Correta\n",
            "\n",
            "\n",
            "Menu:\n",
            "1. Carregar dados\n",
            "2. Filtrar e tratar dados\n",
            "3. Inicializar modelo BERT\n",
            "4. Executar previsÃµes iniciais\n",
            "5. Fine-tuning do modelo\n",
            "6. Executar previsÃµes refinadas\n",
            "0. Sair\n",
            "\n",
            "PrediÃ§Ãµes refinadas (amostra):\n",
            "\n",
            "Pergunta: What is Cars, Energy, Nuclear Diplomacy and the Law: A Reflective Memoir of Three Generations?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Comprehension &amp; Collaboration: Inquiry Circles in Action?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Cressi Playa 2.5mm Men's Front Zip Shorty Wetsuit?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Greenway Long life Multi Stage Filter?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Metropolis 2003?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Travels with Gannon and Wyatt: Great Bear Rainforest (Travels With Gannon &amp; Wyatt)?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Green Lantern Vol. 3: Wanted - Hal Jordan?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Ford Explorer console replacement armrest cover - Medium Graphite Gray?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Ecology?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Magic: the Gathering - Lightning Strike - Theros?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is ?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Redesigning the American Dream: The Future of Housing, Work and Family Life?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Tobit (Anchor Yale Bible Commentaries)?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Buying Your First Horse: A Comprehensive Guide to Preparing For, Finding and Purchasing a Great Horse?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Webkinz Frog?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Stand By Me - Faces Quotes Movie Poster?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is LinenTablecloth 70-Inch Round Polyester Tablecloth Chocolate?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Buffalo Whisker Biscuit Medium Arrow Rest Brush Right and Left Hand - Black for Recurve or Compound Bow?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is The Runaway Jury: A Novel?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Eyourlife New Women Girls Bridal Hot Elastic Faux Wig Hair Bands Plait Bohemian Gypsy Braided Hoop Headbands-White-S?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Space Mission Radio Control Space Shuttle?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is DaVinci Sleigh Multiposition Lock Glider and Ottoman, Beige with Espresso Wood?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Designs for Health - Insomnitol 60 vcaps?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Star Wars Trilogy Bonus Disc (2004)?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is ?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is UNREAL Candy - Chocolate Caramel Peanut Nougat Bar?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is POLARIS 180 / 280 / 380 UWF Restrictor Kit 10-108-00 1010800?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Hacking Electronics: An Illustrated DIY Guide for Makers and Hobbyists?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is ECG Self-Study Book?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Sally Hansen Hard as Nails Color, Rock N' Hard, 0.45 Fluid Ounce?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Fly Creek UL 2 Footprint by Big Agnes?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Switch Machine?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Visiting Hours?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Out Of The Past?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is EFX Silicone Sport Bracelet, 8-Inch, Trans/White?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Improv Wisdom: Don't Prepare, Just Show Up?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is En Passant Par La Demeure (French Edition)?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Moeller A/D Portable Fuel Tank with Handle?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is I Can Too! Boys?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Hydor Centrifical Pump 300 All-Purpose Pump for Aquariums and Terrariums 300 GPH - Original Pico Evolution 1200?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Pentair 472604 Igniter Replacement MiniMax NT STD Pool and Spa Heater?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Ever Ready First Aid Instant Cold Pack, 6x9 Inch (Pack of 24)?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is ?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Of the Laws of Ecclesiastical Polity (Cambridge Texts in the History of Political Thought)?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Le Creuset Stoneware Set of 6 Teaspoons, Truffle?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Anime My Neighbor Totoro iPhone 4/4s Case Hard iPhone 4/4s Fitted Case?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Mastering Machine Applique: The Complete Guide Including: Invisible Machine Applique Satin Stitch Blanket Stitch &amp; Much More?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Victor Roach &amp; Insect Traps &amp; Monitor - 30 Units (60 Traps)?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Mythologies of the World: The Illustrated Guide to Mythological Beliefs &amp; Customs?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Laser Pegs 16-in-1 Predator Building Set?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Samsung&copy; Behold T919 Car Charger?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Set of 4 Vinyl Siding Hooks Hook Shape: Adjustable 'S' Shaped Hooks?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is eoGEAR Top Tube Century Bag Deluxe?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Lexicon Omega Desktop Recording Studio?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is MCTS 70-680 Exam Cram: Microsoft Windows 7, Configuring?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is The Streets of Ankh-Morpork?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is GE Lighting 16703 150-Watt A21 Reveal Reader Light?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Flambeau Predator Masters Series Lone Howler Coyote Decoy?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Agamemnon, The Choephori &amp; The Eumenides (Cliffs Notes)?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Ten Inch Lazy Susan Turntable with Grip Surface?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Teaching with Heart: Poetry that Speaks to the Courage to Teach?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Beatrix Big Kid Nigel Shark Backpack?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is ?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is (Lightahead) Ipad 2 and 3 Case with Built-in Bluetooth Keyboard Leather Cover with Keypad (Pink)?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is ?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Vladimir Horowitz, Complete Masterworks Recordings 1962-1973, Vol. I: The Studio Recordings 1962-63?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is CoverON(TM) Hard BLACK Snap on Rubberized Faceplate Cover Case for SAMSUNG T959 VIBRANT/GALAXY S 4G (T-MOBILE) [WCB432]?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Samsung Camera Cb20u12 USB Cable for Aq100, Tl100, Tl105, Tl110, Tl205, Tl210, Tl220, Tl240, Tl9, Tl90, Hz15w, Hz25w, Hz30w, Hz35w, L100, L110, L200, L210, Nv30, Nv4, Nv40,sl102, Sl105, Sl201, Sl202, Sl310, Sl420, Sl50, Sl502, Sl600, Sl605, Sl620?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Bang Rajan?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Go Power! GP-SW3000-24 3000-Watt Pure Sine Wave Inverter?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Isis in the Ancient World?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is ?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Come to Where I'm From?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Lego Hobbit Dwalin the Dwarf Minifigure?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is JVCC ATG-7502 ATG Tape (Acid Neutral): 1/2 in. x 36 yds. (Clear Adhesive on Gold Liner)?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Feed the Tree / Star / Sexy / Dream on Me?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Amzer 3D Metallic Snap On Case Cover for iPhone 4 and iPhone 4S - Retail Packaging - Cupcake?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Conair Mega Ceramic Hair Brush, Boar Bristle, Round, Small?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is ?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Chitchat Drink Markers?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Mahler - Symphony No. 3?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Corelle Livingware Winter Frost White 30-Piece Dinnerware Set, Service for 6?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Wallpaper* City Guide Barcelona 2012 (Wallpaper City Guides)?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Pelican Large Hardware and Accessory Case with Padded Dividers 1560-004-110?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Koestler: The Literary and Political Odyssey of a Twentieth-Century Skeptic?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Come To Me - Sun's Eye Mystic Blends Oils - 1/2 Ounce Bottle?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Hot Pink Rubberized Snap-On Hard Skin Case Cover for Samsung Fascinate i500 Verizon Phone by Electromaster?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Stens 335-608 Mulching Blade, Replaces Mtd 742-0742A 942-0742A 742-0742 942-0742?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Mexican Days: Journeys into the Heart of Mexico?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Thomas &amp; Betts DH1670L Carlon Wired Push-Button?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Service Dog Leash - Red?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Optimum Nutrition 100% Natural Oats and Whey Milk Chocolate, 3 Pound?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is America By Perry Ellis For Men. Eau De Toilette Spray 5 Ounces?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Shimano 2014 Men's Off-Road Sport Cycling Shoes - SH-M088L?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Weaver SPR 30mm Optic Mount?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is EMPIRE LG Marquee Orchid Safari Stealth Rubberized Design Hard Case Cover [EMPIRE Packaging]?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is The Cocktail Waitress (Hard Case Crime)?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Love to Mom?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Do Androids Dream of Electric Sheep Vol 4?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "Pergunta: What is Star Wars Micro Machines Tie Bomber?\n",
            "Resposta prevista: ğŸ‘ Correta\n",
            "\n",
            "ğŸ“Š AcurÃ¡cia: 1.00 | F1-score: 1.00\n",
            "\n",
            "Menu:\n",
            "1. Carregar dados\n",
            "2. Filtrar e tratar dados\n",
            "3. Inicializar modelo BERT\n",
            "4. Executar previsÃµes iniciais\n",
            "5. Fine-tuning do modelo\n",
            "6. Executar previsÃµes refinadas\n",
            "0. Sair\n"
          ]
        }
      ],
      "source": [
        "# Instalar dependÃªncias necessÃ¡rias\n",
        "!pip install transformers datasets torch\n",
        "!pip install -U transformers\n",
        "\n",
        "import json\n",
        "import html\n",
        "import random\n",
        "import torch\n",
        "import zipfile\n",
        "import gzip\n",
        "import time\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# FunÃ§Ã£o para ler um .json de mesmo nome dentro do .zip\n",
        "def load_data_from_zip_2(zip_file):\n",
        "    zip_base = Path(zip_file).stem\n",
        "    json_filename = f\"{zip_base}.json\"\n",
        "\n",
        "    with zipfile.ZipFile(zip_file, 'r') as z:\n",
        "        if json_filename not in z.namelist():\n",
        "            raise FileNotFoundError(f\"O arquivo {json_filename} nÃ£o foi encontrado dentro do ZIP.\")\n",
        "        with z.open(json_filename) as f:\n",
        "            data = [json.loads(line) for line in f]\n",
        "    return data, data  # usa o mesmo conteÃºdo como treino e teste\n",
        "\n",
        "def load_data_from_zip(zip_file):\n",
        "\n",
        "\n",
        "    with zipfile.ZipFile(zip_file, 'r') as z:\n",
        "        with z.open('LF-Amazon-1.3M/trn.json.gz') as f:\n",
        "            with gzip.open(f, 'rt', encoding='utf-8') as json_file:\n",
        "                train_data = [json.loads(line) for line in json_file]\n",
        "\n",
        "        with z.open('LF-Amazon-1.3M/tst.json.gz') as f:\n",
        "            with gzip.open(f, 'rt', encoding='utf-8') as json_file:\n",
        "                test_data = [json.loads(line) for line in json_file]\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "# Mostrar os dados brutos\n",
        "def display_raw_train_data(train_data):\n",
        "    print(\"\\nPrimeiras 5 linhas do conjunto de treinamento (raw):\")\n",
        "    for item in train_data[:5]:\n",
        "        print(item)\n",
        "\n",
        "# Mostrar exemplos dos dados carregados\n",
        "def display_loaded_examples(train_data, test_data):\n",
        "    print(\"\\nExemplos dos dados carregados:\")\n",
        "    display_raw_train_data(train_data)\n",
        "    print(\"\\nPrimeiras 5 linhas do conjunto de teste:\")\n",
        "    for item in test_data[:5]:\n",
        "        print(item)\n",
        "\n",
        "# PrÃ©-processar dados\n",
        "def preprocess_data(data):\n",
        "    filtered_data = []\n",
        "    for item in data:\n",
        "        title = item.get('title', '').strip()\n",
        "        content = item.get('content', '').strip()\n",
        "        title = html.unescape(title)\n",
        "        content = html.unescape(content)\n",
        "        if title and content:\n",
        "            item['title'] = title.encode('utf-8').decode('utf-8')\n",
        "            item['content'] = content.encode('utf-8').decode('utf-8')\n",
        "            filtered_data.append(item)\n",
        "    return filtered_data\n",
        "\n",
        "# Mostrar dados tratados\n",
        "def display_examples(train_data, test_data):\n",
        "    print(\"\\nExemplos dos dados tratados:\")\n",
        "    for item in train_data[:5]:\n",
        "        print(item)\n",
        "    print(\"\\nPrimeiras 5 linhas do conjunto de teste:\")\n",
        "    for item in test_data[:5]:\n",
        "        print(item)\n",
        "\n",
        "# Inicializa o modelo BERT\n",
        "def initialize_model():\n",
        "    return BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Fine-tuning do modelo\n",
        "def fine_tune_model(model, train_data, sample_size=100_000, epochs=1, batch_size=8):\n",
        "    start = time.time()\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
        "\n",
        "    # Amostragem\n",
        "    sampled = random.sample(train_data, min(sample_size, len(train_data)))\n",
        "\n",
        "    inputs, labels = [], []\n",
        "    for item in sampled:\n",
        "        question = f\"What is {item['title']}?\"\n",
        "        context = item['content'].strip()\n",
        "        if context:\n",
        "            inputs.append(question + \" \" + context)\n",
        "            labels.append(1)\n",
        "\n",
        "    # Pares negativos\n",
        "    negatives = random.sample(sampled, len(inputs))\n",
        "    for item, neg in zip(sampled, negatives):\n",
        "        if item['uid'] != neg['uid']:\n",
        "            question = f\"What is {item['title']}?\"\n",
        "            context = neg['content'].strip()\n",
        "            if context:\n",
        "                inputs.append(question + \" \" + context)\n",
        "                labels.append(0)\n",
        "\n",
        "    # TokenizaÃ§Ã£o\n",
        "    encodings = tokenizer(inputs, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    encodings['labels'] = torch.tensor(labels)\n",
        "\n",
        "    # Dataset customizado\n",
        "    class CustomDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, enc):\n",
        "            self.enc = enc\n",
        "        def __getitem__(self, idx):\n",
        "            return {k: v[idx] for k, v in self.enc.items()}\n",
        "        def __len__(self):\n",
        "            return len(self.enc['input_ids'])\n",
        "\n",
        "    dataset = CustomDataset(encodings)\n",
        "\n",
        "    # MÃ©tricas personalizadas\n",
        "    def compute_metrics(pred):\n",
        "        y_true = pred.label_ids\n",
        "        y_pred = pred.predictions.argmax(-1)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        return {\n",
        "            \"accuracy\": acc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1\n",
        "        }\n",
        "\n",
        "    # ConfiguraÃ§Ã£o do treinamento\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=1500,\n",
        "        save_strategy=\"no\",\n",
        "        disable_tqdm=False,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    end = time.time()\n",
        "    print(f\"\\nâœ… Fine-tuning concluÃ­do em {(end - start)/60:.2f} minutos.\", flush=True)\n",
        "\n",
        "    return model\n",
        "\n",
        "# AvaliaÃ§Ã£o com perguntas reais\n",
        "def execute_prompt(model):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model.eval()\n",
        "\n",
        "    # Get the device of the model\n",
        "    device = model.device\n",
        "\n",
        "    prompts = [\n",
        "        {\n",
        "            \"question\": \"What is Girls Ballet Tutu Neon Pink?\",\n",
        "            \"context\": \"High quality 3 layer ballet tutu. 12 inches in length\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is Adult Ballet Tutu Yellow?\",\n",
        "            \"context\": \"Elegant adult-sized yellow tutu suitable for performances.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is The Way Things Work: An Illustrated Encyclopedia of Technology?\",\n",
        "            \"context\": \"A visual encyclopedia that explains how modern machines function.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is Mog's Kittens?\",\n",
        "            \"context\": \"Judith Kerr's board book for toddlers about the cat Mog.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is Misty of Chincoteague?\",\n",
        "            \"context\": \"A children's novel by Marguerite Henry about a wild pony.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    print(\"\\nResultados do prompt:\")\n",
        "    for item in prompts:\n",
        "        input_text = item['question'] + \" \" + item['context']\n",
        "        inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True)\n",
        "\n",
        "        # Move the input tensors to the same device as the model\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "        print(f\"Pergunta: {item['question']}\")\n",
        "        print(f\"Resposta prevista: {'âœ… Correta' if prediction == 1 else 'âŒ Incorreta'}\\n\")\n",
        "\n",
        "# Rest of the code remains the same...\n",
        "\n",
        "# AvaliaÃ§Ã£o com os dados do teste\n",
        "def refined_predictions(model, test_data, sample_size=100):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model.eval()\n",
        "\n",
        "    # Get the device of the model\n",
        "    device = model.device\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    print(\"\\nPrediÃ§Ãµes refinadas (amostra):\\n\")\n",
        "\n",
        "    sampled = [item for item in test_data if item['content'].strip()]\n",
        "    sampled = random.sample(sampled, min(sample_size, len(sampled)))\n",
        "\n",
        "    for item in sampled:\n",
        "        title = item['title']\n",
        "        content = item['content'].strip()\n",
        "        question = f\"What is {title}?\"\n",
        "\n",
        "        # Simular label: tÃ­tulo + conteÃºdo = correto\n",
        "        expected = 1\n",
        "\n",
        "        input_text = question + \" \" + content\n",
        "        inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True)\n",
        "\n",
        "        # Move the input tensors to the same device as the model\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "\n",
        "        predictions.append(prediction)\n",
        "        references.append(expected)\n",
        "\n",
        "        print(f\"Pergunta: {question}\")\n",
        "        print(f\"Resposta prevista: {'ğŸ‘ Correta' if prediction == 1 else 'ğŸ‘ Incorreta'}\")\n",
        "\n",
        "    from sklearn.metrics import accuracy_score, f1_score\n",
        "    acc = accuracy_score(references, predictions)\n",
        "    f1 = f1_score(references, predictions)\n",
        "\n",
        "    print(f\"\\nğŸ“Š AcurÃ¡cia: {acc:.2f} | F1-score: {f1:.2f}\")\n",
        "\n",
        "\n",
        "# Menu interativo\n",
        "def main_menu():\n",
        "    model = None\n",
        "    filtered_train_data = None\n",
        "    while True:\n",
        "        print(\"\\nMenu:\")\n",
        "        print(\"1. Carregar dados\")\n",
        "        print(\"2. Filtrar e tratar dados\")\n",
        "        print(\"3. Inicializar modelo BERT\")\n",
        "        print(\"4. Executar previsÃµes iniciais\")\n",
        "        print(\"5. Fine-tuning do modelo\")\n",
        "        print(\"6. Executar previsÃµes refinadas\")\n",
        "        print(\"0. Sair\")\n",
        "\n",
        "        choice = input(\"Escolha uma opÃ§Ã£o: \")\n",
        "\n",
        "        if choice == '1':\n",
        "            zip_file = '/content/drive/MyDrive/Colab Notebooks/LF-Amazon-1.3M.raw.zip'\n",
        "            global train_data, test_data\n",
        "            train_data, test_data = load_data_from_zip(zip_file)\n",
        "            display_loaded_examples(train_data, test_data)\n",
        "\n",
        "        elif choice == '2':\n",
        "            filtered_train_data = preprocess_data(train_data)\n",
        "            display_examples(filtered_train_data, test_data)\n",
        "\n",
        "        elif choice == '3':\n",
        "            model = initialize_model()\n",
        "            print(\"Modelo BERT inicializado com sucesso.\")\n",
        "\n",
        "        elif choice == '4':\n",
        "            if model is not None:\n",
        "                execute_prompt(model)\n",
        "            else:\n",
        "                print(\"Modelo nÃ£o inicializado.\")\n",
        "\n",
        "        elif choice == '5':\n",
        "            if model is not None and filtered_train_data:\n",
        "                model = fine_tune_model(model, filtered_train_data)\n",
        "                print(\"Fine-tuning concluÃ­do.\")\n",
        "                execute_prompt(model) # Call execute_prompt after fine-tuning\n",
        "            else:\n",
        "                print(\"Faltam dados ou modelo.\")\n",
        "\n",
        "        elif choice == '6':\n",
        "            if model is not None:\n",
        "                refined_predictions(model, test_data)\n",
        "            else:\n",
        "                print(\"Treine o modelo antes.\")\n",
        "\n",
        "        elif choice == '0':\n",
        "            print(\"Saindo.\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"OpÃ§Ã£o invÃ¡lida.\")\n",
        "\n",
        "# ExecuÃ§Ã£o do menu\n",
        "if __name__ == \"__main__\":\n",
        "    main_menu()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwXyyAMXwI3W",
        "outputId": "c19fbb7c-710a-4ab6-e310-e643dcdc37e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}